# =============================================================================
# Ripple LLM 模型配置文件（示例） / Ripple LLM Config File (Example)
# =============================================================================
#
# 将此文件复制为 llm_config.yaml 并填入你的实际配置。
# Copy this file to llm_config.yaml and fill in your actual config.
# 自动搜索路径：llm_config.yaml / config/llm_config.yaml
# Auto-discovery paths: llm_config.yaml / config/llm_config.yaml
#
# 配置优先级（高 → 低）： / Config priority (high → low):
#   1. 代码传入 llm_config 字典（simulate() 的参数） / llm_config dict passed in code (simulate() param)
#   2. 本配置文件 / This config file
#   3. 环境变量（通过 ${VAR} 在 YAML 中引用） / Env vars (referenced via ${VAR} in YAML)
#
# 没有内置默认模型。所有角色的模型配置必须通过以上三种方式之一提供，
# 否则引擎启动时会抛出 ConfigurationError。
# No built-in default models. All role configs must be provided via one of
# the above methods, or the engine raises ConfigurationError on startup.
#
# 支持 ${ENV_VAR} 语法引用环境变量，支持 ${VAR:-default} 默认值语法。
# Supports ${ENV_VAR} syntax for env vars and ${VAR:-default} for defaults.
#
# api_mode 支持 4 种值： / api_mode supports 4 values:
#   - chat_completions — 标准 OpenAI Chat Completions 格式（默认） / Standard OpenAI Chat Completions format (default)
#   - responses        — OpenAI Responses API 格式 / OpenAI Responses API format
#   - anthropic        — Anthropic Messages API 格式 / Anthropic Messages API format
#   - bedrock          — AWS Bedrock InvokeModel 格式 / AWS Bedrock InvokeModel format
#
# 未显式设置 api_mode 时，根据 platform 和 url 自动推断：
# When api_mode is not set, it is auto-inferred from platform and url:
#   - platform=anthropic 且无自定义 URL → anthropic / platform=anthropic with no custom URL → anthropic
#   - platform=bedrock → bedrock
#   - URL 包含 /responses → responses / URL contains /responses → responses
#   - 其他 → chat_completions / Otherwise → chat_completions
# =============================================================================

# ---------------------------------------------------------------------------
# 全局默认配置（必填） / Global defaults (required)
# 所有角色继承此处的基础配置。角色级配置可覆盖任意字段。
# All roles inherit these base settings. Role-level config can override any field.
# 至少需要指定 model_platform 和 model_name。
# At minimum, model_platform and model_name must be specified.
# ---------------------------------------------------------------------------
_default:
  model_platform: anthropic
  model_name: claude-sonnet-4-20250514     # 全局默认模型 / Global default model
  api_key: ${ANTHROPIC_API_KEY}            # 从环境变量读取 / Read from env var
  # url: https://api.anthropic.com         # 一般不需要显式设置 / Usually not needed explicitly
  # api_mode: anthropic                    # 自动推断，可省略 / Auto-inferred, can be omitted
  temperature: 0.7
  max_retries: 3

# ---------------------------------------------------------------------------
# 角色级配置 / Role-level config
#
# 每个角色可覆盖全局默认的任意字段。 / Each role can override any global default field.
# 支持两种格式： / Two formats supported:
#   - 简写：仅模型名字符串（自动推断 platform） / Shorthand: model name string only (platform auto-inferred)
#   - 完整：字典格式（可设置所有参数） / Full: dict format (all params configurable)
#
# v2 架构使用 3 个角色： / v2 architecture uses 3 roles:
#   omniscient — 全视者（核心编排者，负责初始化、传播裁决、观测、结果合成） / Omniscient (core orchestrator: init, propagation verdict, observation, synthesis)
#   star       — KOL 个体决策（质量优先） / KOL individual decisions (quality-first)
#   sea        — 群体行为（成本优先） / Crowd behavior (cost-first)
# ---------------------------------------------------------------------------

# Omniscient Agent — 全视者（核心编排，需要高智能模型） / Omniscient (core orchestration, needs high-intelligence model)
omniscient:
  model_name: claude-opus-4-6
  temperature: 0.7

# Star Agent — 质量优先（KOL 决策需要高智能模型） / Quality-first (KOL decisions need high-intelligence model)
star:
  model_name: claude-opus-4-6
  temperature: 0.8                         # Star 可适当提高创造性 / Higher creativity for Star

# Sea Agent — 成本优先（群体行为可用轻量模型） / Cost-first (crowd behavior can use lightweight model)
sea:
  model_name: claude-haiku
  temperature: 0.5                         # Sea 偏稳定 / More stable for Sea

# ---------------------------------------------------------------------------
# 嵌入模型配置（可选） / Embedding model config (optional)
#
# 用于 Phase 3 语义相似度过滤： / Used for Phase 3 semantic similarity filtering:
# - Ripple 创建时预计算 content_embedding / Pre-compute content_embedding when Ripple is created
# - Sea Agent 接收涟漪前通过 cosine similarity 预筛选 / Pre-filter via cosine similarity before Sea Agent receives ripples
# - 降低 LLM 调用次数，实现 O(1) 相关性判断 / Reduce LLM calls, achieve O(1) relevance judgment
#
# 不配置或 type=noop 时，直接跳过相似度检测（不会用 LLM 替代），
# 涟漪传播到所有拓扑邻居，完全由 Sea Agent LLM 决策反应。
# When unconfigured or type=noop, similarity check is skipped (NOT replaced by LLM);
# ripples propagate to all topological neighbors, fully decided by Sea Agent LLM.
#
# type 支持两种： / type supports two values:
#   - noop:                 不做嵌入（默认，跳过相似度预筛选） / No embedding (default, skip similarity pre-filtering)
#   - api:                  使用 /embeddings API（OpenAI 兼容或 DashScope） / Use /embeddings API (OpenAI-compatible or DashScope)
# ---------------------------------------------------------------------------
_embedding:
  type: api
  model_name: text-embedding-3-small       # OpenAI 嵌入模型 / OpenAI embedding model
  api_key: ${OPENAI_API_KEY}               # 从环境变量读取 / Read from env var
  # url: https://api.openai.com/v1         # OpenAI 一般不需要显式设置 / Usually not needed for OpenAI
  # dimensions: 512                        # 可选：自定义向量维度 / Optional: custom vector dimensions

# ---------------------------------------------------------------------------
# 降级映射（可选） / Degradation mapping (optional)
#
# 当 LLM 调用次数接近上限（>80%）时，引擎会自动将高成本角色降级到
# 更便宜的模型。降级映射在此定义。未配置的角色不会降级。
# When LLM call count nears the limit (>80%), the engine auto-downgrades
# expensive roles to cheaper models. Unconfigured roles won't degrade.
# ---------------------------------------------------------------------------
_degradation:
  omniscient: claude-sonnet-4-20250514     # Omniscient: Opus → Sonnet（降级 / downgrade）
  star: claude-sonnet-4-20250514           # Star: Opus → Sonnet（降级 / downgrade）


# =============================================================================
# 高级示例：混合 Provider（Anthropic + OpenAI） / Advanced: Mixed providers (Anthropic + OpenAI)
# =============================================================================

# _default:
#   model_platform: anthropic
#   model_name: claude-sonnet-4-20250514
#   api_key: ${ANTHROPIC_API_KEY}

# omniscient:
#   model_platform: openai
#   model_name: gpt-4o
#   api_key: ${OPENAI_API_KEY}
#   url: ${OPENAI_API_BASE_URL:-https://api.openai.com/v1}
#   api_mode: chat_completions
#   temperature: 0.7

# star:
#   model_platform: openai
#   model_name: gpt-4o
#   api_key: ${OPENAI_API_KEY}
#   api_mode: chat_completions
#   temperature: 0.8

# sea:
#   model_platform: openai
#   model_name: gpt-4o-mini
#   api_key: ${OPENAI_API_KEY}
#   api_mode: chat_completions


# =============================================================================
# 高级示例：Azure OpenAI（Chat Completions 模式） / Advanced: Azure OpenAI (Chat Completions mode)
# =============================================================================

# star:
#   model_platform: azure
#   model_name: gpt-4o
#   api_key: ${AZURE_OPENAI_API_KEY}
#   url: https://your-resource.openai.azure.com/
#   api_mode: chat_completions
#   api_version: "2024-08-01-preview"
#   azure_deployment_name: gpt-4o-deployment


# =============================================================================
# 高级示例：自定义代理网关 / OpenAI 兼容 API（Chat Completions 模式）
# Advanced: Custom proxy gateway / OpenAI-compatible API (Chat Completions mode)
# 适用于国内代理、Ollama、vLLM 等 OpenAI 兼容端点
# For domestic proxies, Ollama, vLLM, and other OpenAI-compatible endpoints
# =============================================================================

# _default:
#   model_platform: openai
#   model_name: claude-sonnet-4-20250514
#   url: https://your-proxy.example.com/v1
#   api_key: ${PROXY_API_KEY}
#   api_mode: chat_completions

# star:
#   model_name: claude-3-opus
#   temperature: 0.8

# sea:
#   model_name: gpt-4o-mini
#   temperature: 0.5


# =============================================================================
# 高级示例：火山引擎/豆包（Chat Completions 模式）
# Advanced: Volcengine/Doubao (Chat Completions mode)
#
# 使用标准 /chat/completions 端点，通过 ChatCompletionsAdapter httpx 直连。
# Uses standard /chat/completions endpoint via ChatCompletionsAdapter httpx direct connection.
# =============================================================================

# _default:
#   model_platform: openai
#   model_name: doubao-seed-1-6-flash-250828
#   api_key: ${ARK_API_KEY}
#   url: https://ark.cn-beijing.volces.com/api/v3
#   api_mode: chat_completions                  # 默认值，可省略 / Default, can be omitted
#   temperature: 0.7


# =============================================================================
# 高级示例：火山引擎/豆包（Responses API 模式）
# Advanced: Volcengine/Doubao (Responses API mode)
#
# 使用 /responses 端点（OpenAI Responses API 兼容格式）。
# Uses /responses endpoint (OpenAI Responses API compatible format).
# api_mode 设为 "responses" 时通过 ResponsesAPIAdapter httpx 直连。
# When api_mode is "responses", connects via ResponsesAPIAdapter httpx directly.
#
# Responses API 与 Chat Completions 的区别：
# Differences between Responses API and Chat Completions:
#   - 请求体使用 "input" 替代 "messages" / Request body uses "input" instead of "messages"
#   - 内容类型使用 "input_text"/"input_image" 替代 "text"/"image_url" / Content types use "input_text"/"input_image" instead of "text"/"image_url"
#   - 响应使用 "output"（含 output_text）替代 "choices[].message.content" / Response uses "output" (with output_text) instead of "choices[].message.content"
#   - 支持 instructions 字段传递系统提示词 / Supports "instructions" field for system prompts
# =============================================================================

# _default:
#   model_platform: openai
#   model_name: doubao-seed-1-6-flash-250828
#   api_key: ${ARK_API_KEY}
#   url: https://ark.cn-beijing.volces.com/api/v3
#   api_mode: responses                          # 使用 Responses API 端点 / Use Responses API endpoint
#   temperature: 0.7

# star:
#   model_name: doubao-seed-1-6-250115           # 高质量模型 / High-quality model
#   api_mode: responses
#   temperature: 0.8

# sea:
#   model_name: doubao-seed-1-6-flash-250828     # 轻量快速模型 / Lightweight fast model
#   api_mode: responses
#   temperature: 0.5


# =============================================================================
# 高级示例：混合模式（同时使用 Anthropic 原生 API 和 Responses API）
# Advanced: Mixed mode (Anthropic native API + Responses API)
#
# 不同角色可以使用不同的 api_mode。例如 Omniscient 使用 Anthropic 原生 API，
# 而 Sea 使用火山引擎的 Responses API。
# Different roles can use different api_modes. E.g. Omniscient uses Anthropic
# native API while Sea uses Volcengine Responses API.
# =============================================================================

# _default:
#   model_platform: anthropic
#   model_name: claude-sonnet-4-20250514
#   api_key: ${ANTHROPIC_API_KEY}
#   # api_mode: anthropic                        # 自动推断 / Auto-inferred

# omniscient:
#   model_platform: anthropic
#   model_name: claude-opus-4-6
#   # api_mode: anthropic                        # 自动推断 / Auto-inferred

# sea:
#   model_platform: openai
#   model_name: doubao-seed-1-6-flash-250828
#   api_key: ${ARK_API_KEY}
#   url: https://ark.cn-beijing.volces.com/api/v3
#   api_mode: responses                          # 直连 Responses API / Direct Responses API connection


# =============================================================================
# 高级示例：Azure AI Foundry（Responses API 模式）
# Advanced: Azure AI Foundry (Responses API mode)
#
# Azure Responses API 端点的 URL 为完整路径，包含 api-version 参数。
# Azure Responses API endpoint URL is the full path including api-version param.
# 适配器自动检测 Azure 域名并使用 api-key 认证头（替代 Bearer Token）。
# Adapter auto-detects Azure domains and uses api-key auth header (instead of Bearer Token).
#
# URL 格式支持： / Supported URL formats:
#   - 完整 URL（含路径和 query 参数）： / Full URL (with path and query params):
#     https://xxx.cognitiveservices.azure.com/openai/responses?api-version=2025-04-01-preview
#     → 直接使用，不做任何修改 / Used as-is, no modification
#   - 基础 URL（自动追加 /responses 和 api-version）： / Base URL (auto-appends /responses and api-version):
#     https://xxx.cognitiveservices.azure.com/openai
#     → 自动变为 .../openai/responses?api-version=<api_version> / Auto-becomes .../openai/responses?api-version=<api_version>
# =============================================================================

# omniscient:
#   model_name: gpt-5
#   api_key: ${AZURE_OPENAI_API_KEY}
#   url: https://gpt5codex02-resource.cognitiveservices.azure.com/openai/responses?api-version=2025-04-01-preview
#   api_mode: responses

# --- 或者使用基础 URL + api_version 字段（等效） / Or use base URL + api_version field (equivalent) ---
# omniscient:
#   model_name: gpt-5
#   api_key: ${AZURE_OPENAI_API_KEY}
#   url: https://gpt5codex02-resource.cognitiveservices.azure.com/openai
#   api_mode: responses
#   api_version: "2025-04-01-preview"            # 自动追加到 query 参数 / Auto-appended to query params


# =============================================================================
# 高级示例：AWS Bedrock / Advanced: AWS Bedrock
#
# Bedrock 使用 boto3 + AWS SigV4 签名，需要安装可选依赖：
# Bedrock uses boto3 + AWS SigV4 signing, requires optional dependency:
#   pip install ripple[bedrock]
# 认证通过 AWS 凭证链（环境变量、~/.aws/credentials 等）。
# Auth via AWS credential chain (env vars, ~/.aws/credentials, etc.).
# =============================================================================

# omniscient:
#   model_platform: bedrock
#   model_name: anthropic.claude-3-opus-20240229-v1:0
#   # api_mode: bedrock                          # 自动推断 / Auto-inferred
#   # Bedrock 使用 AWS 凭证，不需要 api_key / Bedrock uses AWS credentials, no api_key needed
#   # 需要配置 AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY 环境变量 / Requires AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY env vars
#   # 可通过 extra 指定 region_name 和 aws_profile： / Optionally specify region_name and aws_profile via extra:
#   # region_name: us-east-1
#   # aws_profile: dev


# =============================================================================
# 嵌入模型高级示例 / Advanced embedding model examples
#
# api_format 字段决定请求/响应格式： / api_format field determines request/response format:
#   - "openai"（默认，可省略）: 标准 OpenAI 格式 / "openai" (default, can be omitted): Standard OpenAI format
#     兼容: OpenAI / 硅基流动 / Azure OpenAI 等 / Compatible: OpenAI / SiliconFlow / Azure OpenAI etc.
#     请求 / Request: {"model": "xxx", "input": ["text"], "encoding_format": "float"}
#     响应 / Response: {"data": [{"embedding": [...], "index": 0}]}
#
#   - "dashscope": 阿里百炼 DashScope 格式 / "dashscope": Alibaba DashScope format
#     请求 / Request: {"model": "xxx", "input": {"texts": ["text"]}}
#     响应 / Response: {"output": {"embeddings": [{"embedding": [...], "text_index": 0}]}}
# =============================================================================

# ---- 示例：硅基流动 SiliconFlow（OpenAI 兼容格式） / Example: SiliconFlow (OpenAI-compatible format) ----
# _embedding:
#   type: api
#   model_name: Qwen/Qwen3-Embedding-8B
#   api_key: ${SILICONFLOW_API_KEY}
#   url: https://api.siliconflow.cn/v1
#   # api_format: openai                   # 默认值，可省略 / Default, can be omitted

# ---- 示例：阿里百炼 DashScope（DashScope 专用格式） / Example: Alibaba DashScope (DashScope-specific format) ----
# _embedding:
#   type: api
#   api_format: dashscope                  # 必须显式指定！ / Must be explicitly set!
#   model_name: text-embedding-v4
#   api_key: ${DASHSCOPE_API_KEY}
#   url: https://dashscope.aliyuncs.com/api/v1/services/embeddings/text-embedding/text-embedding

# ---- 示例：OpenAI 嵌入模型 / Example: OpenAI embedding model ----
# _embedding:
#   type: api
#   model_name: text-embedding-3-small     # 或 text-embedding-3-large / Or text-embedding-3-large
#   api_key: ${OPENAI_API_KEY}
#   # url: https://api.openai.com/v1       # 一般不需要显式设置 / Usually not needed
#   dimensions: 512                        # text-embedding-3 系列支持自定义维度 / text-embedding-3 series supports custom dimensions

# ---- 示例：Azure OpenAI 嵌入模型 / Example: Azure OpenAI embedding model ----
# _embedding:
#   type: api
#   model_name: text-embedding-ada-002
#   api_key: ${AZURE_OPENAI_API_KEY}
#   url: https://your-resource.openai.azure.com/openai/deployments/embedding-deployment/embeddings?api-version=2024-02-01

# ---- 示例：关闭嵌入过滤（默认行为） / Example: Disable embedding filtering (default behavior) ----
# _embedding:
#   type: noop
